loss: 4.357047  [  128/807083]
loss: 4.321827  [12928/807083]
loss: 4.168506  [25728/807083]
loss: 3.959727  [38528/807083]
loss: 3.794081  [51328/807083]
loss: 3.671581  [64128/807083]
loss: 3.595999  [76928/807083]
loss: 3.729280  [89728/807083]
loss: 3.551281  [102528/807083]
loss: 3.339189  [115328/807083]
loss: 3.444793  [128128/807083]
loss: 3.382150  [140928/807083]
loss: 3.407278  [153728/807083]
loss: 3.401610  [166528/807083]
loss: 3.121471  [179328/807083]
loss: 3.178039  [192128/807083]
loss: 3.116268  [204928/807083]
loss: 3.185565  [217728/807083]
loss: 3.092269  [230528/807083]
loss: 2.953789  [243328/807083]
loss: 3.019886  [256128/807083]
loss: 3.040189  [268928/807083]
loss: 3.107300  [281728/807083]
loss: 2.999592  [294528/807083]
loss: 3.111371  [307328/807083]
loss: 2.993201  [320128/807083]
loss: 3.134048  [332928/807083]
loss: 2.997762  [345728/807083]
loss: 2.937347  [358528/807083]
loss: 2.810352  [371328/807083]
loss: 2.822766  [384128/807083]
loss: 2.835721  [396928/807083]
loss: 2.752759  [409728/807083]
loss: 2.840134  [422528/807083]
loss: 2.958003  [435328/807083]
loss: 2.759810  [448128/807083]
loss: 2.531502  [460928/807083]
loss: 2.515035  [473728/807083]
loss: 2.680219  [486528/807083]
loss: 2.675228  [499328/807083]
loss: 2.771480  [512128/807083]
loss: 2.581097  [524928/807083]
loss: 2.764323  [537728/807083]
loss: 2.571724  [550528/807083]
loss: 2.634934  [563328/807083]
loss: 2.641834  [576128/807083]
loss: 2.542720  [588928/807083]
loss: 2.335988  [601728/807083]
loss: 2.676294  [614528/807083]
loss: 2.463643  [627328/807083]
loss: 2.594076  [640128/807083]
loss: 2.499519  [652928/807083]
loss: 2.490428  [665728/807083]
loss: 2.538316  [678528/807083]
loss: 2.533615  [691328/807083]
loss: 2.698474  [704128/807083]
loss: 2.585162  [716928/807083]
loss: 2.453545  [729728/807083]
loss: 2.562584  [742528/807083]
loss: 2.565292  [755328/807083]
loss: 2.367605  [768128/807083]
loss: 2.424703  [780928/807083]
loss: 2.673959  [793728/807083]
loss: 2.559951  [806528/807083]
Epoch 1/5, Training Loss (epoch avg): 2.9588, Test Loss: 2.3086, Test Accuracy: 35.92%
loss: 2.450909  [  128/807083]
loss: 2.468877  [12928/807083]
loss: 2.607630  [25728/807083]
loss: 2.456686  [38528/807083]
loss: 2.506649  [51328/807083]
loss: 2.450219  [64128/807083]
loss: 2.441653  [76928/807083]
loss: 2.286567  [89728/807083]
loss: 2.474812  [102528/807083]
loss: 2.190181  [115328/807083]
loss: 2.611457  [128128/807083]
loss: 2.249361  [140928/807083]
loss: 2.496698  [153728/807083]
loss: 2.587641  [166528/807083]
loss: 2.363268  [179328/807083]
loss: 2.367820  [192128/807083]
loss: 2.378858  [204928/807083]
loss: 2.434264  [217728/807083]
loss: 2.381341  [230528/807083]
loss: 2.252457  [243328/807083]
loss: 2.126001  [256128/807083]
loss: 2.429118  [268928/807083]
loss: 2.274600  [281728/807083]
loss: 2.312172  [294528/807083]
loss: 2.694776  [307328/807083]
loss: 2.349969  [320128/807083]
loss: 2.493329  [332928/807083]
loss: 2.387837  [345728/807083]
loss: 2.571074  [358528/807083]
loss: 2.350712  [371328/807083]
loss: 2.090640  [384128/807083]
loss: 2.296669  [396928/807083]
loss: 2.169756  [409728/807083]
loss: 2.373014  [422528/807083]
loss: 2.348702  [435328/807083]
loss: 2.437381  [448128/807083]
loss: 1.994112  [460928/807083]
loss: 2.062756  [473728/807083]
loss: 2.293601  [486528/807083]
loss: 2.022316  [499328/807083]
loss: 2.522922  [512128/807083]
loss: 2.345831  [524928/807083]
loss: 2.182325  [537728/807083]
loss: 2.275367  [550528/807083]
loss: 2.238189  [563328/807083]
loss: 2.238411  [576128/807083]
loss: 2.120006  [588928/807083]
loss: 1.975971  [601728/807083]
loss: 2.321516  [614528/807083]
loss: 2.209186  [627328/807083]
loss: 2.178802  [640128/807083]
loss: 2.112203  [652928/807083]
loss: 2.154810  [665728/807083]
loss: 2.088549  [678528/807083]
loss: 2.184475  [691328/807083]
loss: 2.360899  [704128/807083]
loss: 2.402980  [716928/807083]
loss: 2.145778  [729728/807083]
loss: 2.308170  [742528/807083]
loss: 2.145573  [755328/807083]
loss: 2.167156  [768128/807083]
loss: 2.145930  [780928/807083]
loss: 2.309881  [793728/807083]
loss: 2.160822  [806528/807083]
Epoch 2/5, Training Loss (epoch avg): 2.2935, Test Loss: 2.0209, Test Accuracy: 43.84%
loss: 2.171009  [  128/807083]
loss: 2.118479  [12928/807083]
loss: 2.365537  [25728/807083]
loss: 2.221612  [38528/807083]
loss: 2.143741  [51328/807083]
loss: 2.250303  [64128/807083]
loss: 2.162004  [76928/807083]
loss: 1.983928  [89728/807083]
loss: 2.152920  [102528/807083]
loss: 1.841410  [115328/807083]
loss: 2.350887  [128128/807083]
loss: 1.888086  [140928/807083]
loss: 2.317060  [153728/807083]
loss: 2.269128  [166528/807083]
loss: 2.004058  [179328/807083]
loss: 2.076075  [192128/807083]
loss: 2.166506  [204928/807083]
loss: 2.106264  [217728/807083]
loss: 2.119149  [230528/807083]
loss: 2.007872  [243328/807083]
loss: 2.055162  [256128/807083]
loss: 2.152475  [268928/807083]
loss: 1.905413  [281728/807083]
loss: 2.039129  [294528/807083]
loss: 2.306130  [307328/807083]
loss: 2.106190  [320128/807083]
loss: 2.161521  [332928/807083]
loss: 2.093827  [345728/807083]
loss: 2.124080  [358528/807083]
loss: 2.061235  [371328/807083]
loss: 1.904418  [384128/807083]
loss: 2.025024  [396928/807083]
loss: 1.886148  [409728/807083]
loss: 2.040823  [422528/807083]
loss: 2.177665  [435328/807083]
loss: 2.174034  [448128/807083]
loss: 1.936795  [460928/807083]
loss: 1.966488  [473728/807083]
loss: 2.150341  [486528/807083]
loss: 1.983481  [499328/807083]
loss: 2.372827  [512128/807083]
loss: 2.188819  [524928/807083]
loss: 2.016331  [537728/807083]
loss: 2.072328  [550528/807083]
loss: 2.000150  [563328/807083]
loss: 2.112309  [576128/807083]
loss: 2.013477  [588928/807083]
loss: 1.929144  [601728/807083]
loss: 2.098409  [614528/807083]
loss: 2.025251  [627328/807083]
loss: 2.102627  [640128/807083]
loss: 2.066930  [652928/807083]
loss: 1.962092  [665728/807083]
loss: 2.062578  [678528/807083]
loss: 2.015509  [691328/807083]
loss: 2.303478  [704128/807083]
loss: 2.206653  [716928/807083]
loss: 1.892701  [729728/807083]
loss: 2.162554  [742528/807083]
loss: 2.078912  [755328/807083]
loss: 2.062088  [768128/807083]
loss: 2.066972  [780928/807083]
loss: 2.247402  [793728/807083]
loss: 1.967874  [806528/807083]
Epoch 3/5, Training Loss (epoch avg): 2.0872, Test Loss: 1.8982, Test Accuracy: 47.29%
loss: 2.089630  [  128/807083]
loss: 2.056219  [12928/807083]
loss: 2.211921  [25728/807083]
loss: 1.955125  [38528/807083]
loss: 2.147899  [51328/807083]
loss: 2.149051  [64128/807083]
loss: 2.028575  [76928/807083]
loss: 1.839411  [89728/807083]
loss: 2.039594  [102528/807083]
loss: 1.795335  [115328/807083]
loss: 2.317824  [128128/807083]
loss: 1.874683  [140928/807083]
loss: 2.344751  [153728/807083]
loss: 2.142940  [166528/807083]
loss: 1.949189  [179328/807083]
loss: 1.965414  [192128/807083]
loss: 1.895093  [204928/807083]
loss: 1.889548  [217728/807083]
loss: 2.080144  [230528/807083]
loss: 1.781866  [243328/807083]
loss: 1.874313  [256128/807083]
loss: 2.064040  [268928/807083]
loss: 1.757440  [281728/807083]
loss: 1.926395  [294528/807083]
loss: 2.057278  [307328/807083]
loss: 2.007286  [320128/807083]
loss: 2.057787  [332928/807083]
loss: 1.915026  [345728/807083]
loss: 2.065044  [358528/807083]
loss: 1.947067  [371328/807083]
loss: 1.878589  [384128/807083]
loss: 1.965276  [396928/807083]
loss: 1.808231  [409728/807083]
loss: 1.873784  [422528/807083]
loss: 2.039157  [435328/807083]
loss: 2.087367  [448128/807083]
loss: 1.725042  [460928/807083]
loss: 1.912946  [473728/807083]
loss: 1.961678  [486528/807083]
loss: 1.771294  [499328/807083]
loss: 2.208809  [512128/807083]
loss: 2.001366  [524928/807083]
loss: 1.924392  [537728/807083]
loss: 1.978964  [550528/807083]
loss: 1.896628  [563328/807083]
loss: 2.112919  [576128/807083]
loss: 1.902062  [588928/807083]
loss: 1.903602  [601728/807083]
loss: 2.032407  [614528/807083]
loss: 1.941116  [627328/807083]
loss: 1.850732  [640128/807083]
loss: 1.876556  [652928/807083]
loss: 1.857829  [665728/807083]
loss: 1.833370  [678528/807083]
loss: 1.870653  [691328/807083]
loss: 2.109151  [704128/807083]
loss: 2.046646  [716928/807083]
loss: 1.879455  [729728/807083]
loss: 2.053051  [742528/807083]
loss: 2.035001  [755328/807083]
loss: 1.915721  [768128/807083]
loss: 1.838866  [780928/807083]
loss: 2.087485  [793728/807083]
loss: 1.921544  [806528/807083]
Epoch 4/5, Training Loss (epoch avg): 1.9752, Test Loss: 1.8318, Test Accuracy: 48.98%
loss: 1.894302  [  128/807083]
loss: 1.949937  [12928/807083]
loss: 2.152359  [25728/807083]
loss: 1.929499  [38528/807083]
loss: 1.955630  [51328/807083]
loss: 2.078680  [64128/807083]
loss: 1.910582  [76928/807083]
loss: 1.903448  [89728/807083]
loss: 2.022386  [102528/807083]
loss: 1.699156  [115328/807083]
loss: 2.274406  [128128/807083]
loss: 1.681452  [140928/807083]
loss: 2.324518  [153728/807083]
loss: 2.075057  [166528/807083]
loss: 1.838205  [179328/807083]
loss: 1.839260  [192128/807083]
loss: 1.863918  [204928/807083]
loss: 1.956492  [217728/807083]
loss: 1.898109  [230528/807083]
loss: 1.886515  [243328/807083]
loss: 1.758078  [256128/807083]
loss: 2.001972  [268928/807083]
loss: 1.733248  [281728/807083]
loss: 1.815788  [294528/807083]
loss: 2.046611  [307328/807083]
loss: 1.937598  [320128/807083]
loss: 2.086601  [332928/807083]
loss: 1.880151  [345728/807083]
loss: 1.954317  [358528/807083]
loss: 1.858007  [371328/807083]
loss: 1.872495  [384128/807083]
loss: 1.870611  [396928/807083]
loss: 1.681186  [409728/807083]
loss: 1.882389  [422528/807083]
loss: 2.119871  [435328/807083]
loss: 1.957486  [448128/807083]
loss: 1.638826  [460928/807083]
loss: 1.809352  [473728/807083]
loss: 1.945555  [486528/807083]
loss: 1.755037  [499328/807083]
loss: 2.219496  [512128/807083]
loss: 1.891123  [524928/807083]
loss: 1.822214  [537728/807083]
loss: 1.989525  [550528/807083]
loss: 1.853285  [563328/807083]
loss: 2.138417  [576128/807083]
loss: 1.774339  [588928/807083]
loss: 1.768083  [601728/807083]
loss: 1.866468  [614528/807083]
loss: 1.803357  [627328/807083]
loss: 1.851960  [640128/807083]
loss: 1.791785  [652928/807083]
loss: 1.816030  [665728/807083]
loss: 1.867607  [678528/807083]
loss: 1.792560  [691328/807083]
loss: 2.143867  [704128/807083]
loss: 2.042058  [716928/807083]
loss: 1.802881  [729728/807083]
loss: 2.114828  [742528/807083]
loss: 1.933759  [755328/807083]
loss: 1.745652  [768128/807083]
loss: 1.927485  [780928/807083]
loss: 2.040403  [793728/807083]
loss: 1.999745  [806528/807083]
Epoch 5/5, Training Loss (epoch avg): 1.9030, Test Loss: 1.7890, Test Accuracy: 50.15%
