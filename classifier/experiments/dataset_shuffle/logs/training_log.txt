loading projection weights from data\word2vec-google-news-300.bin
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from data\\word2vec-google-news-300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-04-01T19:30:16.749819', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}
loss: 2.936480  [   64/984410]
loss: 2.141112  [ 6464/984410]
loss: 1.567867  [12864/984410]
loss: 1.580952  [19264/984410]
loss: 1.760774  [25664/984410]
loss: 1.429106  [32064/984410]
loss: 1.671929  [38464/984410]
loss: 1.469015  [44864/984410]
loss: 1.218955  [51264/984410]
loss: 0.970716  [57664/984410]
loss: 1.225939  [64064/984410]
loss: 1.140368  [70464/984410]
loss: 1.217935  [76864/984410]
loss: 1.015177  [83264/984410]
loss: 1.348445  [89664/984410]
loss: 1.067506  [96064/984410]
loss: 1.046417  [102464/984410]
loss: 0.916039  [108864/984410]
loss: 0.881972  [115264/984410]
loss: 0.988493  [121664/984410]
loss: 0.930632  [128064/984410]
loss: 1.096694  [134464/984410]
loss: 0.838640  [140864/984410]
loss: 1.172113  [147264/984410]
loss: 0.788222  [153664/984410]
loss: 1.235726  [160064/984410]
loss: 1.066371  [166464/984410]
loss: 0.919590  [172864/984410]
loss: 1.103033  [179264/984410]
loss: 1.311673  [185664/984410]
loss: 0.992871  [192064/984410]
loss: 0.931987  [198464/984410]
loss: 1.119399  [204864/984410]
loss: 0.976517  [211264/984410]
loss: 1.108481  [217664/984410]
loss: 1.309815  [224064/984410]
loss: 0.886052  [230464/984410]
loss: 0.977219  [236864/984410]
loss: 1.420215  [243264/984410]
loss: 1.203662  [249664/984410]
loss: 0.973599  [256064/984410]
loss: 0.927251  [262464/984410]
loss: 1.074726  [268864/984410]
loss: 1.268373  [275264/984410]
loss: 0.960905  [281664/984410]
loss: 0.944326  [288064/984410]
loss: 0.995979  [294464/984410]
loss: 0.930372  [300864/984410]
loss: 1.005682  [307264/984410]
loss: 1.052352  [313664/984410]
loss: 1.048325  [320064/984410]
loss: 0.674251  [326464/984410]
loss: 0.834262  [332864/984410]
loss: 0.860444  [339264/984410]
loss: 1.165557  [345664/984410]
loss: 0.892161  [352064/984410]
loss: 1.081631  [358464/984410]
loss: 1.008739  [364864/984410]
loss: 0.899752  [371264/984410]
loss: 0.877713  [377664/984410]
loss: 0.737504  [384064/984410]
loss: 0.940337  [390464/984410]
loss: 0.921478  [396864/984410]
loss: 0.661253  [403264/984410]
loss: 1.158144  [409664/984410]
loss: 0.640851  [416064/984410]
loss: 0.820276  [422464/984410]
loss: 0.895943  [428864/984410]
loss: 0.841362  [435264/984410]
loss: 1.040810  [441664/984410]
loss: 0.784872  [448064/984410]
loss: 1.008950  [454464/984410]
loss: 0.511818  [460864/984410]
loss: 0.972557  [467264/984410]
loss: 1.182804  [473664/984410]
loss: 0.920866  [480064/984410]
loss: 0.966203  [486464/984410]
loss: 0.825281  [492864/984410]
loss: 0.801070  [499264/984410]
loss: 0.733586  [505664/984410]
loss: 0.890733  [512064/984410]
loss: 0.767605  [518464/984410]
loss: 0.861913  [524864/984410]
loss: 1.283623  [531264/984410]
loss: 0.724895  [537664/984410]
loss: 0.570313  [544064/984410]
loss: 0.964889  [550464/984410]
loss: 1.013532  [556864/984410]
loss: 0.870014  [563264/984410]
loss: 0.978109  [569664/984410]
loss: 1.032866  [576064/984410]
loss: 0.584036  [582464/984410]
loss: 0.990612  [588864/984410]
loss: 0.833556  [595264/984410]
loss: 0.564621  [601664/984410]
loss: 1.071983  [608064/984410]
loss: 1.090224  [614464/984410]
loss: 0.822716  [620864/984410]
loss: 0.798295  [627264/984410]
loss: 1.108683  [633664/984410]
loss: 0.999880  [640064/984410]
loss: 1.031224  [646464/984410]
loss: 0.917851  [652864/984410]
loss: 0.998448  [659264/984410]
loss: 1.045996  [665664/984410]
loss: 0.863283  [672064/984410]
loss: 0.672942  [678464/984410]
loss: 0.892284  [684864/984410]
loss: 0.720568  [691264/984410]
loss: 0.915639  [697664/984410]
loss: 0.999236  [704064/984410]
loss: 0.965618  [710464/984410]
loss: 0.567998  [716864/984410]
loss: 0.806316  [723264/984410]
loss: 0.875061  [729664/984410]
loss: 0.833951  [736064/984410]
loss: 0.598356  [742464/984410]
loss: 0.988150  [748864/984410]
loss: 0.855835  [755264/984410]
loss: 1.017633  [761664/984410]
loss: 0.772925  [768064/984410]
loss: 0.801751  [774464/984410]
loss: 0.845111  [780864/984410]
loss: 0.850713  [787264/984410]
loss: 0.931344  [793664/984410]
loss: 0.860815  [800064/984410]
loss: 0.788021  [806464/984410]
loss: 0.884929  [812864/984410]
loss: 0.908725  [819264/984410]
loss: 0.823720  [825664/984410]
loss: 0.839986  [832064/984410]
loss: 0.905792  [838464/984410]
loss: 1.037727  [844864/984410]
loss: 0.940039  [851264/984410]
loss: 0.649026  [857664/984410]
loss: 1.054019  [864064/984410]
loss: 0.668787  [870464/984410]
loss: 0.819713  [876864/984410]
loss: 0.834455  [883264/984410]
loss: 0.681461  [889664/984410]
loss: 0.807665  [896064/984410]
loss: 1.158343  [902464/984410]
loss: 0.841990  [908864/984410]
loss: 0.897050  [915264/984410]
loss: 1.005835  [921664/984410]
loss: 0.665940  [928064/984410]
loss: 0.774053  [934464/984410]
loss: 0.915204  [940864/984410]
loss: 0.938598  [947264/984410]
loss: 0.655708  [953664/984410]
loss: 0.882415  [960064/984410]
loss: 0.834847  [966464/984410]
loss: 1.165976  [972864/984410]
loss: 0.861853  [979264/984410]
Epoch 1/2, Training Loss (epoch avg): 0.9689, Test Loss: 0.7674, Test Accuracy: 75.01%
loss: 0.635854  [   64/984410]
loss: 0.839432  [ 6464/984410]
loss: 0.605039  [12864/984410]
loss: 0.677180  [19264/984410]
loss: 1.146528  [25664/984410]
loss: 0.753931  [32064/984410]
loss: 1.088994  [38464/984410]
loss: 0.779459  [44864/984410]
loss: 0.899152  [51264/984410]
loss: 0.657407  [57664/984410]
loss: 0.831382  [64064/984410]
loss: 0.716349  [70464/984410]
loss: 0.757924  [76864/984410]
loss: 0.667545  [83264/984410]
loss: 1.002689  [89664/984410]
loss: 0.834614  [96064/984410]
loss: 0.887543  [102464/984410]
loss: 0.761385  [108864/984410]
loss: 0.638006  [115264/984410]
loss: 0.717831  [121664/984410]
loss: 0.495246  [128064/984410]
loss: 0.929479  [134464/984410]
loss: 0.686549  [140864/984410]
loss: 0.776515  [147264/984410]
loss: 0.506581  [153664/984410]
loss: 1.126106  [160064/984410]
loss: 0.938191  [166464/984410]
loss: 0.688531  [172864/984410]
loss: 0.677467  [179264/984410]
loss: 1.074471  [185664/984410]
loss: 0.853206  [192064/984410]
loss: 0.691534  [198464/984410]
loss: 0.782104  [204864/984410]
loss: 0.613875  [211264/984410]
loss: 0.804013  [217664/984410]
loss: 0.955578  [224064/984410]
loss: 0.567452  [230464/984410]
loss: 0.619754  [236864/984410]
loss: 1.164263  [243264/984410]
loss: 0.872979  [249664/984410]
loss: 0.727679  [256064/984410]
loss: 0.877837  [262464/984410]
loss: 0.851267  [268864/984410]
loss: 1.130246  [275264/984410]
loss: 0.766605  [281664/984410]
loss: 0.811942  [288064/984410]
loss: 0.771572  [294464/984410]
loss: 0.810318  [300864/984410]
loss: 0.875816  [307264/984410]
loss: 0.675379  [313664/984410]
loss: 0.990844  [320064/984410]
loss: 0.552564  [326464/984410]
loss: 0.539031  [332864/984410]
loss: 0.715333  [339264/984410]
loss: 0.993251  [345664/984410]
loss: 0.806788  [352064/984410]
loss: 0.731857  [358464/984410]
loss: 0.857267  [364864/984410]
loss: 0.648411  [371264/984410]
loss: 0.740449  [377664/984410]
loss: 0.674741  [384064/984410]
loss: 0.788509  [390464/984410]
loss: 0.730308  [396864/984410]
loss: 0.480432  [403264/984410]
loss: 0.814719  [409664/984410]
loss: 0.487782  [416064/984410]
loss: 0.595392  [422464/984410]
loss: 0.832883  [428864/984410]
loss: 0.714950  [435264/984410]
loss: 1.038624  [441664/984410]
loss: 0.735813  [448064/984410]
loss: 0.893926  [454464/984410]
loss: 0.444754  [460864/984410]
loss: 0.916195  [467264/984410]
loss: 0.972279  [473664/984410]
loss: 0.753428  [480064/984410]
loss: 0.838669  [486464/984410]
loss: 0.730924  [492864/984410]
loss: 0.696192  [499264/984410]
loss: 0.655905  [505664/984410]
loss: 0.894377  [512064/984410]
loss: 0.625300  [518464/984410]
loss: 0.734998  [524864/984410]
loss: 1.102817  [531264/984410]
loss: 0.698732  [537664/984410]
loss: 0.546680  [544064/984410]
loss: 0.863705  [550464/984410]
loss: 0.989200  [556864/984410]
loss: 0.718188  [563264/984410]
loss: 0.671764  [569664/984410]
loss: 0.922629  [576064/984410]
loss: 0.508694  [582464/984410]
loss: 0.880234  [588864/984410]
loss: 0.702930  [595264/984410]
loss: 0.535676  [601664/984410]
loss: 0.841373  [608064/984410]
loss: 0.817650  [614464/984410]
loss: 0.695064  [620864/984410]
loss: 0.681392  [627264/984410]
loss: 1.051255  [633664/984410]
loss: 0.824440  [640064/984410]
loss: 0.782643  [646464/984410]
loss: 0.896964  [652864/984410]
loss: 0.962137  [659264/984410]
loss: 0.981756  [665664/984410]
loss: 0.748191  [672064/984410]
loss: 0.561903  [678464/984410]
loss: 0.787428  [684864/984410]
loss: 0.694596  [691264/984410]
loss: 0.802463  [697664/984410]
loss: 0.917439  [704064/984410]
loss: 0.784435  [710464/984410]
loss: 0.568254  [716864/984410]
loss: 0.806009  [723264/984410]
loss: 0.817117  [729664/984410]
loss: 0.737372  [736064/984410]
loss: 0.648142  [742464/984410]
loss: 0.985427  [748864/984410]
loss: 0.721230  [755264/984410]
loss: 0.938630  [761664/984410]
loss: 0.702678  [768064/984410]
loss: 0.657934  [774464/984410]
loss: 0.669961  [780864/984410]
loss: 0.726558  [787264/984410]
loss: 0.878235  [793664/984410]
loss: 0.811118  [800064/984410]
loss: 0.552484  [806464/984410]
loss: 0.728339  [812864/984410]
loss: 0.700728  [819264/984410]
loss: 0.780494  [825664/984410]
loss: 0.741732  [832064/984410]
loss: 0.799625  [838464/984410]
loss: 0.953166  [844864/984410]
loss: 1.046156  [851264/984410]
loss: 0.534051  [857664/984410]
loss: 0.986780  [864064/984410]
loss: 0.680478  [870464/984410]
loss: 0.849549  [876864/984410]
loss: 0.772772  [883264/984410]
loss: 0.680613  [889664/984410]
loss: 0.716904  [896064/984410]
loss: 1.075974  [902464/984410]
loss: 0.807886  [908864/984410]
loss: 0.927622  [915264/984410]
loss: 0.935352  [921664/984410]
loss: 0.586080  [928064/984410]
loss: 0.781086  [934464/984410]
loss: 0.821631  [940864/984410]
loss: 0.752380  [947264/984410]
loss: 0.568114  [953664/984410]
loss: 0.907802  [960064/984410]
loss: 0.743146  [966464/984410]
loss: 1.092736  [972864/984410]
loss: 0.835369  [979264/984410]
Epoch 2/2, Training Loss (epoch avg): 0.7783, Test Loss: 0.7315, Test Accuracy: 76.03%
