loading projection weights from data\word2vec-google-news-300.bin
KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from data\\word2vec-google-news-300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-03-28T23:21:43.258443', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}
loss: 2.947078  [   64/984410]
loss: 2.323916  [ 6464/984410]
loss: 2.025928  [12864/984410]
loss: 1.603182  [19264/984410]
loss: 1.577890  [25664/984410]
loss: 1.646661  [32064/984410]
loss: 1.676367  [38464/984410]
loss: 1.866424  [44864/984410]
loss: 1.490084  [51264/984410]
loss: 1.368633  [57664/984410]
loss: 1.404062  [64064/984410]
loss: 1.548395  [70464/984410]
loss: 1.374771  [76864/984410]
loss: 1.335506  [83264/984410]
loss: 1.301698  [89664/984410]
loss: 0.960029  [96064/984410]
loss: 0.989509  [102464/984410]
loss: 1.256405  [108864/984410]
loss: 1.106760  [115264/984410]
loss: 1.499034  [121664/984410]
loss: 1.195284  [128064/984410]
loss: 0.939709  [134464/984410]
loss: 0.900757  [140864/984410]
loss: 1.329843  [147264/984410]
loss: 0.949269  [153664/984410]
loss: 1.510178  [160064/984410]
loss: 1.320065  [166464/984410]
loss: 1.084601  [172864/984410]
loss: 0.814532  [179264/984410]
loss: 0.990281  [185664/984410]
loss: 0.807030  [192064/984410]
loss: 0.984418  [198464/984410]
loss: 1.050309  [204864/984410]
loss: 0.967464  [211264/984410]
loss: 1.121971  [217664/984410]
loss: 1.078201  [224064/984410]
loss: 1.025319  [230464/984410]
loss: 1.356283  [236864/984410]
loss: 1.066819  [243264/984410]
loss: 1.194893  [249664/984410]
loss: 1.220305  [256064/984410]
loss: 1.138989  [262464/984410]
loss: 0.991709  [268864/984410]
loss: 1.342479  [275264/984410]
loss: 0.898420  [281664/984410]
loss: 1.038151  [288064/984410]
loss: 0.917303  [294464/984410]
loss: 1.023453  [300864/984410]
loss: 1.071506  [307264/984410]
loss: 1.020514  [313664/984410]
loss: 0.996560  [320064/984410]
loss: 0.936861  [326464/984410]
loss: 0.951376  [332864/984410]
loss: 1.174409  [339264/984410]
loss: 1.003176  [345664/984410]
loss: 1.089291  [352064/984410]
loss: 1.191411  [358464/984410]
loss: 0.902447  [364864/984410]
loss: 1.068291  [371264/984410]
loss: 0.921443  [377664/984410]
loss: 1.233418  [384064/984410]
loss: 0.870661  [390464/984410]
loss: 1.094327  [396864/984410]
loss: 1.066071  [403264/984410]
loss: 0.937987  [409664/984410]
loss: 0.958677  [416064/984410]
loss: 0.928284  [422464/984410]
loss: 1.432857  [428864/984410]
loss: 0.704086  [435264/984410]
loss: 1.133254  [441664/984410]
loss: 1.242546  [448064/984410]
loss: 0.937319  [454464/984410]
loss: 1.276982  [460864/984410]
loss: 1.265963  [467264/984410]
loss: 0.961396  [473664/984410]
loss: 1.036252  [480064/984410]
loss: 1.303862  [486464/984410]
loss: 0.933513  [492864/984410]
loss: 1.359448  [499264/984410]
loss: 0.611893  [505664/984410]
loss: 0.721983  [512064/984410]
loss: 1.323659  [518464/984410]
loss: 0.812311  [524864/984410]
loss: 1.152825  [531264/984410]
loss: 1.249804  [537664/984410]
loss: 1.296554  [544064/984410]
loss: 1.354727  [550464/984410]
loss: 0.756107  [556864/984410]
loss: 0.938205  [563264/984410]
loss: 0.846875  [569664/984410]
loss: 1.296786  [576064/984410]
loss: 0.965756  [582464/984410]
loss: 1.188651  [588864/984410]
loss: 0.852877  [595264/984410]
loss: 0.778170  [601664/984410]
loss: 0.778402  [608064/984410]
loss: 1.281059  [614464/984410]
loss: 0.991653  [620864/984410]
loss: 1.158901  [627264/984410]
loss: 0.876867  [633664/984410]
loss: 1.223247  [640064/984410]
loss: 0.950683  [646464/984410]
loss: 1.126035  [652864/984410]
loss: 1.029946  [659264/984410]
loss: 1.009852  [665664/984410]
loss: 1.159806  [672064/984410]
loss: 1.116152  [678464/984410]
loss: 1.080226  [684864/984410]
loss: 1.411127  [691264/984410]
loss: 1.017774  [697664/984410]
loss: 0.822230  [704064/984410]
loss: 0.753248  [710464/984410]
loss: 1.114993  [716864/984410]
loss: 1.323024  [723264/984410]
loss: 1.118539  [729664/984410]
loss: 0.977084  [736064/984410]
loss: 1.004390  [742464/984410]
loss: 1.137563  [748864/984410]
loss: 0.914038  [755264/984410]
loss: 1.068995  [761664/984410]
loss: 0.883988  [768064/984410]
loss: 1.004596  [774464/984410]
loss: 0.827439  [780864/984410]
loss: 0.925117  [787264/984410]
loss: 1.062074  [793664/984410]
loss: 0.936465  [800064/984410]
loss: 0.798836  [806464/984410]
loss: 1.067157  [812864/984410]
loss: 0.986264  [819264/984410]
loss: 1.016002  [825664/984410]
loss: 0.718166  [832064/984410]
loss: 1.070443  [838464/984410]
loss: 0.814330  [844864/984410]
loss: 0.689760  [851264/984410]
loss: 0.787962  [857664/984410]
loss: 0.808484  [864064/984410]
loss: 1.345004  [870464/984410]
loss: 0.789836  [876864/984410]
loss: 0.933312  [883264/984410]
loss: 1.142099  [889664/984410]
loss: 0.791917  [896064/984410]
loss: 0.816210  [902464/984410]
loss: 1.223570  [908864/984410]
loss: 1.045486  [915264/984410]
loss: 0.834457  [921664/984410]
loss: 0.944517  [928064/984410]
loss: 0.884904  [934464/984410]
loss: 0.736163  [940864/984410]
loss: 0.950183  [947264/984410]
loss: 0.824475  [953664/984410]
loss: 0.975057  [960064/984410]
loss: 0.817283  [966464/984410]
loss: 0.839995  [972864/984410]
loss: 0.695096  [979264/984410]
Epoch 1/2, Training Loss (epoch avg): 1.0926, Test Loss: 1.0602, Test Accuracy: 69.67%
loss: 0.742410  [   64/984410]
loss: 0.891720  [ 6464/984410]
loss: 0.729755  [12864/984410]
loss: 0.948941  [19264/984410]
loss: 1.095892  [25664/984410]
loss: 0.692684  [32064/984410]
loss: 0.819774  [38464/984410]
loss: 0.800285  [44864/984410]
loss: 1.144292  [51264/984410]
loss: 0.755301  [57664/984410]
loss: 0.785256  [64064/984410]
loss: 0.783300  [70464/984410]
loss: 0.973825  [76864/984410]
loss: 0.704658  [83264/984410]
loss: 0.999531  [89664/984410]
loss: 0.984688  [96064/984410]
loss: 0.977528  [102464/984410]
loss: 0.859243  [108864/984410]
loss: 1.016375  [115264/984410]
loss: 1.092225  [121664/984410]
loss: 1.072383  [128064/984410]
loss: 1.081266  [134464/984410]
loss: 0.900341  [140864/984410]
loss: 1.011236  [147264/984410]
loss: 0.800135  [153664/984410]
loss: 0.855948  [160064/984410]
loss: 1.076217  [166464/984410]
loss: 0.844462  [172864/984410]
loss: 0.903444  [179264/984410]
loss: 0.829063  [185664/984410]
loss: 0.967247  [192064/984410]
loss: 0.871815  [198464/984410]
loss: 0.757184  [204864/984410]
loss: 1.131882  [211264/984410]
loss: 0.935837  [217664/984410]
loss: 0.850619  [224064/984410]
loss: 1.318959  [230464/984410]
loss: 0.905553  [236864/984410]
loss: 0.835651  [243264/984410]
loss: 0.713863  [249664/984410]
loss: 0.907172  [256064/984410]
loss: 0.874707  [262464/984410]
loss: 1.221787  [268864/984410]
loss: 0.765051  [275264/984410]
loss: 1.063798  [281664/984410]
loss: 0.921592  [288064/984410]
loss: 0.888590  [294464/984410]
loss: 1.015571  [300864/984410]
loss: 0.760903  [307264/984410]
loss: 0.870994  [313664/984410]
loss: 0.982386  [320064/984410]
loss: 0.891543  [326464/984410]
loss: 1.057288  [332864/984410]
loss: 0.753011  [339264/984410]
loss: 0.907270  [345664/984410]
loss: 0.954665  [352064/984410]
loss: 0.917338  [358464/984410]
loss: 0.724657  [364864/984410]
loss: 0.903813  [371264/984410]
loss: 0.955591  [377664/984410]
loss: 1.362475  [384064/984410]
loss: 0.549589  [390464/984410]
loss: 1.311427  [396864/984410]
loss: 0.850643  [403264/984410]
loss: 0.851522  [409664/984410]
loss: 1.434555  [416064/984410]
loss: 0.978904  [422464/984410]
loss: 0.787184  [428864/984410]
loss: 0.991970  [435264/984410]
loss: 0.897819  [441664/984410]
loss: 0.630202  [448064/984410]
loss: 1.039007  [454464/984410]
loss: 0.959048  [460864/984410]
loss: 0.862112  [467264/984410]
loss: 0.792593  [473664/984410]
loss: 0.544528  [480064/984410]
loss: 1.126357  [486464/984410]
loss: 1.081317  [492864/984410]
loss: 0.825113  [499264/984410]
loss: 0.915489  [505664/984410]
loss: 0.631426  [512064/984410]
loss: 0.751114  [518464/984410]
loss: 0.850668  [524864/984410]
loss: 1.155672  [531264/984410]
loss: 0.899828  [537664/984410]
loss: 0.850607  [544064/984410]
loss: 0.715935  [550464/984410]
loss: 1.133215  [556864/984410]
loss: 0.967032  [563264/984410]
loss: 1.037767  [569664/984410]
loss: 0.944645  [576064/984410]
loss: 0.884479  [582464/984410]
loss: 0.903172  [588864/984410]
loss: 0.713674  [595264/984410]
loss: 0.750231  [601664/984410]
loss: 0.703849  [608064/984410]
loss: 1.022120  [614464/984410]
loss: 1.045010  [620864/984410]
loss: 1.022691  [627264/984410]
loss: 1.100702  [633664/984410]
loss: 0.889457  [640064/984410]
loss: 1.043670  [646464/984410]
loss: 0.814179  [652864/984410]
loss: 0.976284  [659264/984410]
loss: 1.040696  [665664/984410]
loss: 0.849424  [672064/984410]
loss: 0.986347  [678464/984410]
loss: 0.974610  [684864/984410]
loss: 0.738648  [691264/984410]
loss: 1.294950  [697664/984410]
loss: 0.977340  [704064/984410]
loss: 0.774546  [710464/984410]
loss: 0.932841  [716864/984410]
loss: 0.832865  [723264/984410]
loss: 0.690315  [729664/984410]
loss: 0.739261  [736064/984410]
loss: 0.930794  [742464/984410]
loss: 1.010556  [748864/984410]
loss: 1.145566  [755264/984410]
loss: 1.085788  [761664/984410]
loss: 0.862176  [768064/984410]
loss: 0.574027  [774464/984410]
loss: 0.891851  [780864/984410]
loss: 0.878790  [787264/984410]
loss: 1.217832  [793664/984410]
loss: 0.794130  [800064/984410]
loss: 0.894067  [806464/984410]
loss: 1.173909  [812864/984410]
loss: 0.926767  [819264/984410]
loss: 0.707204  [825664/984410]
loss: 0.970575  [832064/984410]
loss: 0.742927  [838464/984410]
loss: 0.798198  [844864/984410]
loss: 0.759537  [851264/984410]
loss: 1.060478  [857664/984410]
loss: 0.780855  [864064/984410]
loss: 0.908720  [870464/984410]
loss: 0.796676  [876864/984410]
loss: 1.343207  [883264/984410]
loss: 0.740674  [889664/984410]
loss: 0.685602  [896064/984410]
loss: 1.009266  [902464/984410]
loss: 0.829602  [908864/984410]
loss: 0.549541  [915264/984410]
loss: 0.711370  [921664/984410]
loss: 0.490595  [928064/984410]
loss: 0.940585  [934464/984410]
loss: 0.956633  [940864/984410]
loss: 0.717313  [947264/984410]
loss: 0.743163  [953664/984410]
loss: 1.282951  [960064/984410]
loss: 0.953424  [966464/984410]
loss: 0.887816  [972864/984410]
loss: 0.878164  [979264/984410]
Epoch 2/2, Training Loss (epoch avg): 0.9011, Test Loss: 0.9459, Test Accuracy: 71.66%
