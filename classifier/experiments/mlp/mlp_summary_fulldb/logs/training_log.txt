loss: 2.987770  [  128/986091]
loss: 1.957866  [12928/986091]
loss: 1.563811  [25728/986091]
loss: 1.430021  [38528/986091]
loss: 1.643229  [51328/986091]
loss: 1.183087  [64128/986091]
loss: 1.241379  [76928/986091]
loss: 1.272094  [89728/986091]
loss: 1.177488  [102528/986091]
loss: 1.067904  [115328/986091]
loss: 1.019096  [128128/986091]
loss: 1.052722  [140928/986091]
loss: 0.957300  [153728/986091]
loss: 0.836806  [166528/986091]
loss: 0.929016  [179328/986091]
loss: 1.007892  [192128/986091]
loss: 0.877370  [204928/986091]
loss: 0.754679  [217728/986091]
loss: 0.735337  [230528/986091]
loss: 0.992301  [243328/986091]
loss: 0.786084  [256128/986091]
loss: 0.633485  [268928/986091]
loss: 0.698650  [281728/986091]
loss: 0.902294  [294528/986091]
loss: 0.636417  [307328/986091]
loss: 0.778646  [320128/986091]
loss: 0.678827  [332928/986091]
loss: 0.819540  [345728/986091]
loss: 0.600101  [358528/986091]
loss: 0.841693  [371328/986091]
loss: 0.850310  [384128/986091]
loss: 0.712808  [396928/986091]
loss: 0.794877  [409728/986091]
loss: 0.773770  [422528/986091]
loss: 0.909502  [435328/986091]
loss: 0.719893  [448128/986091]
loss: 0.798570  [460928/986091]
loss: 0.730820  [473728/986091]
loss: 0.782875  [486528/986091]
loss: 0.673902  [499328/986091]
loss: 0.779645  [512128/986091]
loss: 0.566239  [524928/986091]
loss: 0.613602  [537728/986091]
loss: 0.743721  [550528/986091]
loss: 0.612129  [563328/986091]
loss: 0.827673  [576128/986091]
loss: 0.780165  [588928/986091]
loss: 0.489854  [601728/986091]
loss: 0.900183  [614528/986091]
loss: 0.747392  [627328/986091]
loss: 0.642148  [640128/986091]
loss: 0.717303  [652928/986091]
loss: 0.606364  [665728/986091]
loss: 0.624229  [678528/986091]
loss: 0.713084  [691328/986091]
loss: 0.729991  [704128/986091]
loss: 0.685783  [716928/986091]
loss: 0.767509  [729728/986091]
loss: 0.789408  [742528/986091]
loss: 0.756878  [755328/986091]
loss: 0.952916  [768128/986091]
loss: 0.746383  [780928/986091]
loss: 0.686655  [793728/986091]
loss: 0.611549  [806528/986091]
loss: 0.582384  [819328/986091]
loss: 0.643343  [832128/986091]
loss: 0.637665  [844928/986091]
loss: 0.799076  [857728/986091]
loss: 0.666088  [870528/986091]
loss: 0.637438  [883328/986091]
loss: 0.658907  [896128/986091]
loss: 0.645106  [908928/986091]
loss: 0.655803  [921728/986091]
loss: 0.726833  [934528/986091]
loss: 0.825803  [947328/986091]
loss: 0.604629  [960128/986091]
loss: 0.782093  [972928/986091]
loss: 0.691522  [985728/986091]
Epoch 1/5, Training Loss (epoch avg): 0.8213, Test Loss: 0.6015, Test Accuracy: 79.58%
loss: 0.671155  [  128/986091]
loss: 0.708176  [12928/986091]
loss: 0.523239  [25728/986091]
loss: 0.642654  [38528/986091]
loss: 0.863660  [51328/986091]
loss: 0.621644  [64128/986091]
loss: 0.639250  [76928/986091]
loss: 0.810933  [89728/986091]
loss: 0.744475  [102528/986091]
loss: 0.835167  [115328/986091]
loss: 0.703234  [128128/986091]
loss: 0.774921  [140928/986091]
loss: 0.728743  [153728/986091]
loss: 0.590180  [166528/986091]
loss: 0.609690  [179328/986091]
loss: 0.638170  [192128/986091]
loss: 0.683916  [204928/986091]
loss: 0.521811  [217728/986091]
loss: 0.517029  [230528/986091]
loss: 0.578251  [243328/986091]
loss: 0.580576  [256128/986091]
loss: 0.501100  [268928/986091]
loss: 0.576667  [281728/986091]
loss: 0.631960  [294528/986091]
loss: 0.530171  [307328/986091]
loss: 0.565061  [320128/986091]
loss: 0.440428  [332928/986091]
loss: 0.639068  [345728/986091]
loss: 0.538675  [358528/986091]
loss: 0.762839  [371328/986091]
loss: 0.694368  [384128/986091]
loss: 0.533295  [396928/986091]
loss: 0.628229  [409728/986091]
loss: 0.586226  [422528/986091]
loss: 0.729665  [435328/986091]
loss: 0.465682  [448128/986091]
loss: 0.646740  [460928/986091]
loss: 0.591891  [473728/986091]
loss: 0.664935  [486528/986091]
loss: 0.564478  [499328/986091]
loss: 0.597966  [512128/986091]
loss: 0.509087  [524928/986091]
loss: 0.541885  [537728/986091]
loss: 0.696868  [550528/986091]
loss: 0.525518  [563328/986091]
loss: 0.701309  [576128/986091]
loss: 0.665389  [588928/986091]
loss: 0.398880  [601728/986091]
loss: 0.719551  [614528/986091]
loss: 0.667104  [627328/986091]
loss: 0.525622  [640128/986091]
loss: 0.647488  [652928/986091]
loss: 0.564718  [665728/986091]
loss: 0.523461  [678528/986091]
loss: 0.616451  [691328/986091]
loss: 0.610357  [704128/986091]
loss: 0.575309  [716928/986091]
loss: 0.652738  [729728/986091]
loss: 0.658108  [742528/986091]
loss: 0.687651  [755328/986091]
loss: 0.772348  [768128/986091]
loss: 0.561429  [780928/986091]
loss: 0.573237  [793728/986091]
loss: 0.559949  [806528/986091]
loss: 0.527407  [819328/986091]
loss: 0.560728  [832128/986091]
loss: 0.512940  [844928/986091]
loss: 0.619425  [857728/986091]
loss: 0.603200  [870528/986091]
loss: 0.567152  [883328/986091]
loss: 0.556194  [896128/986091]
loss: 0.614157  [908928/986091]
loss: 0.515476  [921728/986091]
loss: 0.681791  [934528/986091]
loss: 0.720482  [947328/986091]
loss: 0.566383  [960128/986091]
loss: 0.697859  [972928/986091]
loss: 0.550958  [985728/986091]
Epoch 2/5, Training Loss (epoch avg): 0.5924, Test Loss: 0.5505, Test Accuracy: 80.93%
loss: 0.615093  [  128/986091]
loss: 0.670617  [12928/986091]
loss: 0.490693  [25728/986091]
loss: 0.572407  [38528/986091]
loss: 0.723609  [51328/986091]
loss: 0.507603  [64128/986091]
loss: 0.521901  [76928/986091]
loss: 0.702458  [89728/986091]
loss: 0.662802  [102528/986091]
loss: 0.769655  [115328/986091]
loss: 0.613729  [128128/986091]
loss: 0.640734  [140928/986091]
loss: 0.662166  [153728/986091]
loss: 0.473615  [166528/986091]
loss: 0.575541  [179328/986091]
loss: 0.544409  [192128/986091]
loss: 0.561926  [204928/986091]
loss: 0.474510  [217728/986091]
loss: 0.490504  [230528/986091]
loss: 0.479812  [243328/986091]
loss: 0.565894  [256128/986091]
loss: 0.427220  [268928/986091]
loss: 0.526382  [281728/986091]
loss: 0.584934  [294528/986091]
loss: 0.517687  [307328/986091]
loss: 0.583261  [320128/986091]
loss: 0.481512  [332928/986091]
loss: 0.572233  [345728/986091]
loss: 0.576997  [358528/986091]
loss: 0.751889  [371328/986091]
loss: 0.585841  [384128/986091]
loss: 0.488033  [396928/986091]
loss: 0.519221  [409728/986091]
loss: 0.472202  [422528/986091]
loss: 0.613262  [435328/986091]
loss: 0.469295  [448128/986091]
loss: 0.600744  [460928/986091]
loss: 0.558408  [473728/986091]
loss: 0.596567  [486528/986091]
loss: 0.457365  [499328/986091]
loss: 0.594570  [512128/986091]
loss: 0.421701  [524928/986091]
loss: 0.532613  [537728/986091]
loss: 0.649195  [550528/986091]
loss: 0.484904  [563328/986091]
loss: 0.679431  [576128/986091]
loss: 0.638736  [588928/986091]
loss: 0.366485  [601728/986091]
loss: 0.739934  [614528/986091]
loss: 0.545824  [627328/986091]
loss: 0.443153  [640128/986091]
loss: 0.662965  [652928/986091]
loss: 0.504589  [665728/986091]
loss: 0.500909  [678528/986091]
loss: 0.598992  [691328/986091]
loss: 0.557557  [704128/986091]
loss: 0.542433  [716928/986091]
loss: 0.610745  [729728/986091]
loss: 0.632362  [742528/986091]
loss: 0.627991  [755328/986091]
loss: 0.720265  [768128/986091]
loss: 0.513145  [780928/986091]
loss: 0.500651  [793728/986091]
loss: 0.532836  [806528/986091]
loss: 0.510255  [819328/986091]
loss: 0.537275  [832128/986091]
loss: 0.461373  [844928/986091]
loss: 0.604509  [857728/986091]
loss: 0.610097  [870528/986091]
loss: 0.535928  [883328/986091]
loss: 0.529681  [896128/986091]
loss: 0.525469  [908928/986091]
loss: 0.480106  [921728/986091]
loss: 0.549564  [934528/986091]
loss: 0.691906  [947328/986091]
loss: 0.558473  [960128/986091]
loss: 0.671762  [972928/986091]
loss: 0.571963  [985728/986091]
Epoch 3/5, Training Loss (epoch avg): 0.5424, Test Loss: 0.5335, Test Accuracy: 81.41%
loss: 0.611884  [  128/986091]
loss: 0.707473  [12928/986091]
loss: 0.409555  [25728/986091]
loss: 0.491839  [38528/986091]
loss: 0.673133  [51328/986091]
loss: 0.493887  [64128/986091]
loss: 0.499156  [76928/986091]
loss: 0.586049  [89728/986091]
loss: 0.605728  [102528/986091]
loss: 0.693999  [115328/986091]
loss: 0.611126  [128128/986091]
loss: 0.709597  [140928/986091]
loss: 0.660330  [153728/986091]
loss: 0.526844  [166528/986091]
loss: 0.528928  [179328/986091]
loss: 0.486055  [192128/986091]
loss: 0.572909  [204928/986091]
loss: 0.465068  [217728/986091]
loss: 0.400800  [230528/986091]
loss: 0.508412  [243328/986091]
loss: 0.512132  [256128/986091]
loss: 0.473715  [268928/986091]
loss: 0.546863  [281728/986091]
loss: 0.534964  [294528/986091]
loss: 0.506831  [307328/986091]
loss: 0.472378  [320128/986091]
loss: 0.450076  [332928/986091]
loss: 0.535678  [345728/986091]
loss: 0.476462  [358528/986091]
loss: 0.632245  [371328/986091]
loss: 0.575427  [384128/986091]
loss: 0.414798  [396928/986091]
loss: 0.491425  [409728/986091]
loss: 0.491183  [422528/986091]
loss: 0.628866  [435328/986091]
loss: 0.397167  [448128/986091]
loss: 0.596343  [460928/986091]
loss: 0.583490  [473728/986091]
loss: 0.534966  [486528/986091]
loss: 0.413926  [499328/986091]
loss: 0.549235  [512128/986091]
loss: 0.414077  [524928/986091]
loss: 0.484979  [537728/986091]
loss: 0.629238  [550528/986091]
loss: 0.444457  [563328/986091]
loss: 0.628978  [576128/986091]
loss: 0.583205  [588928/986091]
loss: 0.379976  [601728/986091]
loss: 0.694292  [614528/986091]
loss: 0.586582  [627328/986091]
loss: 0.534531  [640128/986091]
loss: 0.586475  [652928/986091]
loss: 0.534232  [665728/986091]
loss: 0.495181  [678528/986091]
loss: 0.493350  [691328/986091]
loss: 0.527894  [704128/986091]
loss: 0.489521  [716928/986091]
loss: 0.523272  [729728/986091]
loss: 0.577075  [742528/986091]
loss: 0.516417  [755328/986091]
loss: 0.639141  [768128/986091]
loss: 0.452328  [780928/986091]
loss: 0.501993  [793728/986091]
loss: 0.444961  [806528/986091]
loss: 0.471399  [819328/986091]
loss: 0.532803  [832128/986091]
loss: 0.465017  [844928/986091]
loss: 0.592966  [857728/986091]
loss: 0.655790  [870528/986091]
loss: 0.537169  [883328/986091]
loss: 0.543402  [896128/986091]
loss: 0.457644  [908928/986091]
loss: 0.517906  [921728/986091]
loss: 0.600030  [934528/986091]
loss: 0.588002  [947328/986091]
loss: 0.513084  [960128/986091]
loss: 0.642136  [972928/986091]
loss: 0.512930  [985728/986091]
Epoch 4/5, Training Loss (epoch avg): 0.5143, Test Loss: 0.5266, Test Accuracy: 81.68%
loss: 0.570859  [  128/986091]
loss: 0.665958  [12928/986091]
loss: 0.397267  [25728/986091]
loss: 0.466888  [38528/986091]
loss: 0.729531  [51328/986091]
loss: 0.457976  [64128/986091]
loss: 0.504292  [76928/986091]
loss: 0.591682  [89728/986091]
loss: 0.624111  [102528/986091]
loss: 0.661232  [115328/986091]
loss: 0.517793  [128128/986091]
loss: 0.653574  [140928/986091]
loss: 0.577053  [153728/986091]
loss: 0.437175  [166528/986091]
loss: 0.476988  [179328/986091]
loss: 0.474880  [192128/986091]
loss: 0.548242  [204928/986091]
loss: 0.458685  [217728/986091]
loss: 0.376601  [230528/986091]
loss: 0.427391  [243328/986091]
loss: 0.601027  [256128/986091]
loss: 0.436362  [268928/986091]
loss: 0.499560  [281728/986091]
loss: 0.545691  [294528/986091]
loss: 0.484100  [307328/986091]
loss: 0.450392  [320128/986091]
loss: 0.386687  [332928/986091]
loss: 0.520448  [345728/986091]
loss: 0.489371  [358528/986091]
loss: 0.639112  [371328/986091]
loss: 0.523015  [384128/986091]
loss: 0.376575  [396928/986091]
loss: 0.504489  [409728/986091]
loss: 0.480693  [422528/986091]
loss: 0.610016  [435328/986091]
loss: 0.439457  [448128/986091]
loss: 0.515870  [460928/986091]
loss: 0.584249  [473728/986091]
loss: 0.535167  [486528/986091]
loss: 0.484662  [499328/986091]
loss: 0.583909  [512128/986091]
loss: 0.421258  [524928/986091]
loss: 0.492209  [537728/986091]
loss: 0.604237  [550528/986091]
loss: 0.408357  [563328/986091]
loss: 0.597397  [576128/986091]
loss: 0.552589  [588928/986091]
loss: 0.350760  [601728/986091]
loss: 0.624518  [614528/986091]
loss: 0.508215  [627328/986091]
loss: 0.542942  [640128/986091]
loss: 0.557254  [652928/986091]
loss: 0.464758  [665728/986091]
loss: 0.455235  [678528/986091]
loss: 0.505046  [691328/986091]
loss: 0.541903  [704128/986091]
loss: 0.563127  [716928/986091]
loss: 0.567209  [729728/986091]
loss: 0.519683  [742528/986091]
loss: 0.579058  [755328/986091]
loss: 0.647357  [768128/986091]
loss: 0.458448  [780928/986091]
loss: 0.485532  [793728/986091]
loss: 0.496768  [806528/986091]
loss: 0.517435  [819328/986091]
loss: 0.452671  [832128/986091]
loss: 0.437663  [844928/986091]
loss: 0.532005  [857728/986091]
loss: 0.624378  [870528/986091]
loss: 0.485888  [883328/986091]
loss: 0.491819  [896128/986091]
loss: 0.460888  [908928/986091]
loss: 0.448617  [921728/986091]
loss: 0.590270  [934528/986091]
loss: 0.573533  [947328/986091]
loss: 0.488141  [960128/986091]
loss: 0.589802  [972928/986091]
loss: 0.473258  [985728/986091]
Epoch 5/5, Training Loss (epoch avg): 0.4939, Test Loss: 0.5256, Test Accuracy: 81.75%
