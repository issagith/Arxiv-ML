loss: 2.993011  [  128/79428]
loss: 2.681240  [12928/79428]
loss: 2.194220  [25728/79428]
loss: 2.054525  [38528/79428]
loss: 1.955972  [51328/79428]
loss: 1.990168  [64128/79428]
loss: 1.723723  [76928/79428]
Epoch 1/5, Training Loss (epoch avg): 2.2326, Test Loss: 1.6730, Test Accuracy: 42.01%
loss: 2.050221  [  128/79428]
loss: 1.870897  [12928/79428]
loss: 1.410923  [25728/79428]
loss: 1.365900  [38528/79428]
loss: 1.403655  [51328/79428]
loss: 1.473427  [64128/79428]
loss: 1.430164  [76928/79428]
Epoch 2/5, Training Loss (epoch avg): 1.5408, Test Loss: 1.2798, Test Accuracy: 56.22%
loss: 1.570709  [  128/79428]
loss: 1.434383  [12928/79428]
loss: 1.076355  [25728/79428]
loss: 1.102129  [38528/79428]
loss: 1.091349  [51328/79428]
loss: 1.241420  [64128/79428]
loss: 1.109807  [76928/79428]
Epoch 3/5, Training Loss (epoch avg): 1.2514, Test Loss: 1.1282, Test Accuracy: 61.52%
loss: 1.175754  [  128/79428]
loss: 1.286811  [12928/79428]
loss: 0.931011  [25728/79428]
loss: 1.073200  [38528/79428]
loss: 1.002960  [51328/79428]
loss: 1.012231  [64128/79428]
loss: 0.991037  [76928/79428]
Epoch 4/5, Training Loss (epoch avg): 1.1023, Test Loss: 1.0515, Test Accuracy: 64.50%
loss: 1.154276  [  128/79428]
loss: 1.168143  [12928/79428]
loss: 0.710531  [25728/79428]
loss: 0.928008  [38528/79428]
loss: 0.934025  [51328/79428]
loss: 0.882000  [64128/79428]
loss: 0.986298  [76928/79428]
Epoch 5/5, Training Loss (epoch avg): 1.0026, Test Loss: 1.0012, Test Accuracy: 66.60%
