loss: 2.999788  [  128/79428]
loss: 2.869408  [12928/79428]
loss: 2.584753  [25728/79428]
loss: 2.425699  [38528/79428]
loss: 2.280939  [51328/79428]
loss: 2.264209  [64128/79428]
loss: 2.229817  [76928/79428]
Epoch 1/5, Training Loss (epoch avg): 2.5138, Test Loss: 2.0548, Test Accuracy: 33.23%
loss: 2.303950  [  128/79428]
loss: 2.008919  [12928/79428]
loss: 1.892738  [25728/79428]
loss: 1.927780  [38528/79428]
loss: 1.740597  [51328/79428]
loss: 1.915046  [64128/79428]
loss: 1.756238  [76928/79428]
Epoch 2/5, Training Loss (epoch avg): 1.9447, Test Loss: 1.7594, Test Accuracy: 42.44%
loss: 1.931226  [  128/79428]
loss: 1.665161  [12928/79428]
loss: 1.549815  [25728/79428]
loss: 1.656515  [38528/79428]
loss: 1.545520  [51328/79428]
loss: 1.650712  [64128/79428]
loss: 1.635081  [76928/79428]
Epoch 3/5, Training Loss (epoch avg): 1.6890, Test Loss: 1.6068, Test Accuracy: 47.34%
loss: 1.672611  [  128/79428]
loss: 1.524870  [12928/79428]
loss: 1.337643  [25728/79428]
loss: 1.477504  [38528/79428]
loss: 1.312900  [51328/79428]
loss: 1.537147  [64128/79428]
loss: 1.488239  [76928/79428]
Epoch 4/5, Training Loss (epoch avg): 1.5223, Test Loss: 1.5202, Test Accuracy: 50.22%
loss: 1.517018  [  128/79428]
loss: 1.423116  [12928/79428]
loss: 1.214938  [25728/79428]
loss: 1.414262  [38528/79428]
loss: 1.206145  [51328/79428]
loss: 1.390325  [64128/79428]
loss: 1.386059  [76928/79428]
Epoch 5/5, Training Loss (epoch avg): 1.4049, Test Loss: 1.4538, Test Accuracy: 52.61%
