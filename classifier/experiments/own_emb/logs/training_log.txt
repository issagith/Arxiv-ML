loss: 2.958608  [  128/984410]
loss: 2.251570  [12928/984410]
loss: 2.132338  [25728/984410]
loss: 1.793614  [38528/984410]
loss: 1.705083  [51328/984410]
loss: 1.759035  [64128/984410]
loss: 1.489007  [76928/984410]
loss: 1.612860  [89728/984410]
loss: 1.496500  [102528/984410]
loss: 1.485271  [115328/984410]
loss: 1.703359  [128128/984410]
loss: 1.578506  [140928/984410]
loss: 1.313264  [153728/984410]
loss: 1.603830  [166528/984410]
loss: 1.354429  [179328/984410]
loss: 1.262362  [192128/984410]
loss: 1.081898  [204928/984410]
loss: 1.168393  [217728/984410]
loss: 1.227420  [230528/984410]
loss: 1.340636  [243328/984410]
loss: 1.340906  [256128/984410]
loss: 1.324205  [268928/984410]
loss: 1.194845  [281728/984410]
loss: 0.994079  [294528/984410]
loss: 1.313348  [307328/984410]
loss: 1.219231  [320128/984410]
loss: 1.076903  [332928/984410]
loss: 1.192748  [345728/984410]
loss: 1.321708  [358528/984410]
loss: 1.457694  [371328/984410]
loss: 1.100082  [384128/984410]
loss: 1.190748  [396928/984410]
loss: 1.084127  [409728/984410]
loss: 1.031609  [422528/984410]
loss: 1.214383  [435328/984410]
loss: 1.162367  [448128/984410]
loss: 1.483611  [460928/984410]
loss: 1.159950  [473728/984410]
loss: 1.363469  [486528/984410]
loss: 1.315555  [499328/984410]
loss: 1.010073  [512128/984410]
loss: 0.999266  [524928/984410]
loss: 0.927349  [537728/984410]
loss: 0.997233  [550528/984410]
loss: 1.228399  [563328/984410]
loss: 1.072072  [576128/984410]
loss: 1.067189  [588928/984410]
loss: 1.072785  [601728/984410]
loss: 1.051962  [614528/984410]
loss: 1.085924  [627328/984410]
loss: 1.215360  [640128/984410]
loss: 0.908435  [652928/984410]
loss: 0.871346  [665728/984410]
loss: 1.248808  [678528/984410]
loss: 0.957817  [691328/984410]
loss: 1.006133  [704128/984410]
loss: 1.086838  [716928/984410]
loss: 1.117455  [729728/984410]
loss: 1.066545  [742528/984410]
loss: 1.213864  [755328/984410]
loss: 1.095319  [768128/984410]
loss: 0.832999  [780928/984410]
loss: 1.033132  [793728/984410]
loss: 0.881748  [806528/984410]
loss: 1.056631  [819328/984410]
loss: 1.150085  [832128/984410]
loss: 1.089763  [844928/984410]
loss: 1.150835  [857728/984410]
loss: 0.909207  [870528/984410]
loss: 1.277556  [883328/984410]
loss: 1.033857  [896128/984410]
loss: 0.899417  [908928/984410]
loss: 1.136093  [921728/984410]
loss: 0.949821  [934528/984410]
loss: 1.026838  [947328/984410]
loss: 1.153055  [960128/984410]
loss: 1.213182  [972928/984410]
Epoch 1/5, Training Loss (epoch avg): 1.2080, Test Loss: 0.9283, Test Accuracy: 70.71%
loss: 1.076530  [  128/984410]
loss: 0.937009  [12928/984410]
loss: 1.044715  [25728/984410]
loss: 0.970610  [38528/984410]
loss: 0.889247  [51328/984410]
loss: 0.958631  [64128/984410]
loss: 0.816046  [76928/984410]
loss: 0.966581  [89728/984410]
loss: 0.930648  [102528/984410]
loss: 1.011574  [115328/984410]
loss: 1.077846  [128128/984410]
loss: 1.071164  [140928/984410]
loss: 0.881490  [153728/984410]
loss: 1.151976  [166528/984410]
loss: 0.881101  [179328/984410]
loss: 0.901173  [192128/984410]
loss: 0.676233  [204928/984410]
loss: 0.818481  [217728/984410]
loss: 0.818364  [230528/984410]
loss: 1.090882  [243328/984410]
loss: 1.021006  [256128/984410]
loss: 1.054770  [268928/984410]
loss: 0.787535  [281728/984410]
loss: 0.769555  [294528/984410]
loss: 0.892944  [307328/984410]
loss: 0.899989  [320128/984410]
loss: 0.814200  [332928/984410]
loss: 1.002805  [345728/984410]
loss: 1.100126  [358528/984410]
loss: 1.199106  [371328/984410]
loss: 0.942123  [384128/984410]
loss: 0.806620  [396928/984410]
loss: 0.865582  [409728/984410]
loss: 0.895097  [422528/984410]
loss: 0.969541  [435328/984410]
loss: 0.951276  [448128/984410]
loss: 1.255207  [460928/984410]
loss: 0.917695  [473728/984410]
loss: 1.189968  [486528/984410]
loss: 1.089716  [499328/984410]
loss: 0.809586  [512128/984410]
loss: 0.836037  [524928/984410]
loss: 0.817114  [537728/984410]
loss: 0.827417  [550528/984410]
loss: 1.079738  [563328/984410]
loss: 0.930614  [576128/984410]
loss: 0.899141  [588928/984410]
loss: 0.959734  [601728/984410]
loss: 0.964513  [614528/984410]
loss: 0.821983  [627328/984410]
loss: 0.967771  [640128/984410]
loss: 0.770137  [652928/984410]
loss: 0.761917  [665728/984410]
loss: 1.124208  [678528/984410]
loss: 0.921819  [691328/984410]
loss: 0.926870  [704128/984410]
loss: 0.904444  [716928/984410]
loss: 1.036689  [729728/984410]
loss: 0.821639  [742528/984410]
loss: 1.104949  [755328/984410]
loss: 0.935681  [768128/984410]
loss: 0.737633  [780928/984410]
loss: 0.945111  [793728/984410]
loss: 0.697759  [806528/984410]
loss: 1.004908  [819328/984410]
loss: 1.034832  [832128/984410]
loss: 0.909614  [844928/984410]
loss: 1.031897  [857728/984410]
loss: 0.781947  [870528/984410]
loss: 1.132318  [883328/984410]
loss: 0.914397  [896128/984410]
loss: 0.837796  [908928/984410]
loss: 0.957878  [921728/984410]
loss: 0.796600  [934528/984410]
loss: 0.972633  [947328/984410]
loss: 1.018530  [960128/984410]
loss: 1.067001  [972928/984410]
Epoch 2/5, Training Loss (epoch avg): 0.9089, Test Loss: 0.8377, Test Accuracy: 73.24%
loss: 0.973206  [  128/984410]
loss: 0.795251  [12928/984410]
loss: 0.944236  [25728/984410]
loss: 0.918676  [38528/984410]
loss: 0.781947  [51328/984410]
loss: 0.824431  [64128/984410]
loss: 0.773639  [76928/984410]
loss: 0.938433  [89728/984410]
loss: 0.788379  [102528/984410]
loss: 0.877710  [115328/984410]
loss: 0.998387  [128128/984410]
loss: 1.032199  [140928/984410]
loss: 0.809752  [153728/984410]
loss: 1.041434  [166528/984410]
loss: 0.802027  [179328/984410]
loss: 0.871261  [192128/984410]
loss: 0.613408  [204928/984410]
loss: 0.704896  [217728/984410]
loss: 0.764355  [230528/984410]
loss: 0.933752  [243328/984410]
loss: 0.904141  [256128/984410]
loss: 0.963123  [268928/984410]
loss: 0.714582  [281728/984410]
loss: 0.655563  [294528/984410]
loss: 0.814078  [307328/984410]
loss: 0.781077  [320128/984410]
loss: 0.725606  [332928/984410]
loss: 0.893501  [345728/984410]
loss: 1.068625  [358528/984410]
loss: 1.114389  [371328/984410]
loss: 0.892271  [384128/984410]
loss: 0.745472  [396928/984410]
loss: 0.769422  [409728/984410]
loss: 0.866398  [422528/984410]
loss: 0.900908  [435328/984410]
loss: 0.805890  [448128/984410]
loss: 1.058219  [460928/984410]
loss: 0.785298  [473728/984410]
loss: 1.016763  [486528/984410]
loss: 0.907438  [499328/984410]
loss: 0.722879  [512128/984410]
loss: 0.725961  [524928/984410]
loss: 0.636365  [537728/984410]
loss: 0.777033  [550528/984410]
loss: 0.988655  [563328/984410]
loss: 0.859688  [576128/984410]
loss: 0.727251  [588928/984410]
loss: 0.827857  [601728/984410]
loss: 0.784093  [614528/984410]
loss: 0.770924  [627328/984410]
loss: 0.909096  [640128/984410]
loss: 0.666829  [652928/984410]
loss: 0.763452  [665728/984410]
loss: 1.085885  [678528/984410]
loss: 0.846753  [691328/984410]
loss: 0.895459  [704128/984410]
loss: 0.845497  [716928/984410]
loss: 0.928933  [729728/984410]
loss: 0.759407  [742528/984410]
loss: 1.068533  [755328/984410]
loss: 0.851122  [768128/984410]
loss: 0.684859  [780928/984410]
loss: 0.917493  [793728/984410]
loss: 0.649800  [806528/984410]
loss: 0.871557  [819328/984410]
loss: 1.025761  [832128/984410]
loss: 0.834384  [844928/984410]
loss: 0.954215  [857728/984410]
loss: 0.671284  [870528/984410]
loss: 1.022246  [883328/984410]
loss: 0.826876  [896128/984410]
loss: 0.749290  [908928/984410]
loss: 0.958869  [921728/984410]
loss: 0.766490  [934528/984410]
loss: 0.927114  [947328/984410]
loss: 0.962867  [960128/984410]
loss: 1.044112  [972928/984410]
Epoch 3/5, Training Loss (epoch avg): 0.8319, Test Loss: 0.8007, Test Accuracy: 74.21%
loss: 0.879145  [  128/984410]
loss: 0.755524  [12928/984410]
loss: 0.874102  [25728/984410]
loss: 0.907317  [38528/984410]
loss: 0.797240  [51328/984410]
loss: 0.792233  [64128/984410]
loss: 0.764704  [76928/984410]
loss: 0.911960  [89728/984410]
loss: 0.705000  [102528/984410]
loss: 0.833749  [115328/984410]
loss: 0.885370  [128128/984410]
loss: 1.004855  [140928/984410]
loss: 0.760003  [153728/984410]
loss: 0.974335  [166528/984410]
loss: 0.713952  [179328/984410]
loss: 0.795402  [192128/984410]
loss: 0.606064  [204928/984410]
loss: 0.639318  [217728/984410]
loss: 0.708905  [230528/984410]
loss: 0.883828  [243328/984410]
loss: 0.859494  [256128/984410]
loss: 0.861123  [268928/984410]
loss: 0.719724  [281728/984410]
loss: 0.640382  [294528/984410]
loss: 0.777467  [307328/984410]
loss: 0.771370  [320128/984410]
loss: 0.721704  [332928/984410]
loss: 0.839473  [345728/984410]
loss: 0.967709  [358528/984410]
loss: 1.080408  [371328/984410]
loss: 0.856179  [384128/984410]
loss: 0.758465  [396928/984410]
loss: 0.727345  [409728/984410]
loss: 0.858355  [422528/984410]
loss: 0.850069  [435328/984410]
loss: 0.852235  [448128/984410]
loss: 0.994681  [460928/984410]
loss: 0.822361  [473728/984410]
loss: 1.036470  [486528/984410]
loss: 0.911564  [499328/984410]
loss: 0.695503  [512128/984410]
loss: 0.750697  [524928/984410]
loss: 0.662165  [537728/984410]
loss: 0.716591  [550528/984410]
loss: 0.918333  [563328/984410]
loss: 0.755912  [576128/984410]
loss: 0.827636  [588928/984410]
loss: 0.832548  [601728/984410]
loss: 0.744294  [614528/984410]
loss: 0.730231  [627328/984410]
loss: 0.842556  [640128/984410]
loss: 0.715589  [652928/984410]
loss: 0.746046  [665728/984410]
loss: 0.955137  [678528/984410]
loss: 0.778281  [691328/984410]
loss: 0.886453  [704128/984410]
loss: 0.773516  [716928/984410]
loss: 0.921664  [729728/984410]
loss: 0.750887  [742528/984410]
loss: 1.008120  [755328/984410]
loss: 0.814876  [768128/984410]
loss: 0.613862  [780928/984410]
loss: 0.916126  [793728/984410]
loss: 0.633311  [806528/984410]
loss: 0.901096  [819328/984410]
loss: 1.003917  [832128/984410]
loss: 0.757011  [844928/984410]
loss: 0.941834  [857728/984410]
loss: 0.667472  [870528/984410]
loss: 1.008575  [883328/984410]
loss: 0.845555  [896128/984410]
loss: 0.697932  [908928/984410]
loss: 0.815150  [921728/984410]
loss: 0.763454  [934528/984410]
loss: 0.817015  [947328/984410]
loss: 0.938304  [960128/984410]
loss: 1.039719  [972928/984410]
Epoch 4/5, Training Loss (epoch avg): 0.7907, Test Loss: 0.7808, Test Accuracy: 74.80%
loss: 0.823077  [  128/984410]
loss: 0.653601  [12928/984410]
loss: 0.884721  [25728/984410]
loss: 0.864734  [38528/984410]
loss: 0.747012  [51328/984410]
loss: 0.826914  [64128/984410]
loss: 0.674172  [76928/984410]
loss: 0.930569  [89728/984410]
loss: 0.651959  [102528/984410]
loss: 0.825993  [115328/984410]
loss: 0.854009  [128128/984410]
loss: 0.940389  [140928/984410]
loss: 0.754927  [153728/984410]
loss: 0.879517  [166528/984410]
loss: 0.710173  [179328/984410]
loss: 0.708113  [192128/984410]
loss: 0.535181  [204928/984410]
loss: 0.643569  [217728/984410]
loss: 0.735713  [230528/984410]
loss: 0.955824  [243328/984410]
loss: 0.873323  [256128/984410]
loss: 0.921438  [268928/984410]
loss: 0.679072  [281728/984410]
loss: 0.624146  [294528/984410]
loss: 0.696291  [307328/984410]
loss: 0.751982  [320128/984410]
loss: 0.699632  [332928/984410]
loss: 0.798288  [345728/984410]
loss: 0.837633  [358528/984410]
loss: 0.911220  [371328/984410]
loss: 0.841287  [384128/984410]
loss: 0.696552  [396928/984410]
loss: 0.719082  [409728/984410]
loss: 0.798670  [422528/984410]
loss: 0.824264  [435328/984410]
loss: 0.754493  [448128/984410]
loss: 0.923896  [460928/984410]
loss: 0.827250  [473728/984410]
loss: 0.999015  [486528/984410]
loss: 0.917941  [499328/984410]
loss: 0.699944  [512128/984410]
loss: 0.716951  [524928/984410]
loss: 0.632444  [537728/984410]
loss: 0.669979  [550528/984410]
loss: 0.927834  [563328/984410]
loss: 0.725554  [576128/984410]
loss: 0.784999  [588928/984410]
loss: 0.804591  [601728/984410]
loss: 0.753976  [614528/984410]
loss: 0.677911  [627328/984410]
loss: 0.834436  [640128/984410]
loss: 0.699337  [652928/984410]
loss: 0.691857  [665728/984410]
loss: 0.985921  [678528/984410]
loss: 0.705421  [691328/984410]
loss: 0.809772  [704128/984410]
loss: 0.846516  [716928/984410]
loss: 0.922526  [729728/984410]
loss: 0.752279  [742528/984410]
loss: 0.976425  [755328/984410]
loss: 0.812810  [768128/984410]
loss: 0.601983  [780928/984410]
loss: 0.917125  [793728/984410]
loss: 0.597435  [806528/984410]
loss: 0.816773  [819328/984410]
loss: 1.035888  [832128/984410]
loss: 0.770410  [844928/984410]
loss: 0.901990  [857728/984410]
loss: 0.631697  [870528/984410]
loss: 0.993765  [883328/984410]
loss: 0.785855  [896128/984410]
loss: 0.708665  [908928/984410]
loss: 0.748899  [921728/984410]
loss: 0.735293  [934528/984410]
loss: 0.795736  [947328/984410]
loss: 0.966443  [960128/984410]
loss: 0.924795  [972928/984410]
Epoch 5/5, Training Loss (epoch avg): 0.7621, Test Loss: 0.7677, Test Accuracy: 75.10%
